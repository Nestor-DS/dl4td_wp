{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Basics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías ocupadas\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# scikit-learn para la manipulación de datos y métricas\n",
    "from sklearn.model_selection import train_test_split   # Para dividir los datos en conjuntos de entrenamiento y prueba\n",
    "from sklearn.datasets import load_iris                 # Para cargar el conjunto de datos de Iris\n",
    "from sklearn.metrics import roc_auc_score              # Para calcular el ROC AUC\n",
    "\n",
    "# Importar módulos de PyTorch para la creación y entrenamiento de redes neuronales\n",
    "import torch                       # Biblioteca PyTorch\n",
    "import torch.nn as nn              # Módulo de redes neuronales\n",
    "import torch.nn.functional as F    # Funciones de activación y otras operaciones para redes neuronales\n",
    "import torch.optim as optim        # Algoritmos de optimización\n",
    "from torch.utils.data import DataLoader, TensorDataset  # Para crear datasets y data loaders personalizados\n",
    "\n",
    "# Importar módulos de imbalanced-learn para manejar el desbalance de clases\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Importar módulos específicos de Lumin para la configuración de gráficos\n",
    "from lumin.plotting.plot_settings import PlotSettings\n",
    "\n",
    "# Importar módulos adicionales\n",
    "from pathlib import Path  # Para manipulación de rutas de archivos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de estilo de seaborn para la visualización\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración para las gráficas\n",
    "plot_settings = PlotSettings(cat_palette='tab10', savepath=Path('.'), format='.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular el puntaje de prueba\n",
    "def score_test_df(df:pd.DataFrame, cut:float, public_wgt_factor:float=1, private_wgt_factor:float=1, verbose:bool=True):\n",
    "    \"\"\"\n",
    "    Calcula el puntaje de prueba de un DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con las predicciones y otros datos relevantes.\n",
    "        cut (float): Umbral de corte para las predicciones.\n",
    "        public_wgt_factor (float): Factor de peso para el cálculo del AMS público.\n",
    "        private_wgt_factor (float): Factor de peso para el cálculo del AMS privado.\n",
    "        verbose (bool): Indica si se mostrará información detallada.\n",
    "    \n",
    "    Returns:\n",
    "        public_ams (float): Puntaje AMS público.\n",
    "        private_ams (float): Puntaje AMS privado.\n",
    "    \"\"\"\n",
    "    accept = (df.pred >= cut)\n",
    "    signal = (df.gen_target == 1)\n",
    "    bkg = (df.gen_target == 0)\n",
    "    public = (df.private == 0)\n",
    "    private = (df.private == 1)\n",
    "\n",
    "    public_ams = calc_ams(public_wgt_factor*np.sum(df.loc[accept & public & signal, 'gen_weight']),\n",
    "                          public_wgt_factor*np.sum(df.loc[accept & public & bkg, 'gen_weight']))\n",
    "\n",
    "    private_ams = calc_ams(private_wgt_factor*np.sum(df.loc[accept & private & signal, 'gen_weight']),\n",
    "                           private_wgt_factor*np.sum(df.loc[accept & private & bkg, 'gen_weight']))\n",
    "\n",
    "    if verbose: print(\"Public:Private AMS: {} : {}\".format(public_ams, private_ams))    \n",
    "    return public_ams, private_ams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo pre-entrenado ResNet-18\n",
    "model_res = models.resnet18(pretrained=True)\n",
    "\n",
    "# Obtener el número de características en la capa de clasificación del modelo pre-entrenado\n",
    "num_features = model_res.fc.in_features\n",
    "\n",
    "# Reemplazar la capa de clasificación (fully connected) con una nueva capa lineal para adaptarla a un problema de clasificación con 3 clases\n",
    "model_res.fc = nn.Linear(num_features, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, dataset_sizes, criterion, optimizer, device, num_epochs, scheduler = None):\n",
    "    \"\"\"\n",
    "    Función para entrenar un modelo de red neuronal.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Modelo de red neuronal a entrenar.\n",
    "        dataloaders (dict): Diccionario que contiene los dataloaders para entrenamiento y validación.\n",
    "        dataset_sizes (dict): Diccionario que contiene el tamaño de los conjuntos de datos de entrenamiento y validación.\n",
    "        criterion: Función de pérdida.\n",
    "        optimizer: Optimizador.\n",
    "        device: Dispositivo de cómputo (CPU o GPU) para entrenamiento.\n",
    "        scheduler: (Opcional) Scheduler para ajustar la tasa de aprendizaje.\n",
    "        num_epochs (int): Número de épocas de entrenamiento (por defecto: 1).\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): Modelo entrenado.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"EPOCH {epoch + 1}/{num_epochs}:\")\n",
    "        print(\"-\" * 10)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Set model mode (train/eval)\n",
    "            model.train(phase == 'train')\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # Backward pass + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train' and scheduler:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Calculate epoch loss and accuracy\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "            \n",
    "            # Deep copy the model if validation accuracy improves\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_weights = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Training completed in {(total_time // 60):.0f}m {(total_time % 60):.0f}s\")\n",
    "    print(f\"BEST VALIDATION ACCURACY: {best_acc:.4f}\")\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_model_Vit(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    best_val_accuracy = 0.0  # Variable para almacenar la mejor precisión de validación\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device, dtype=torch.long)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"Train Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device, dtype=torch.long)\n",
    "                outputs = model(images).logits\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            val_accuracy = correct / total\n",
    "            print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "\n",
    "        print('-' * 10)\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "    print(f\"BEST VALIDATION ACCURACY: {best_val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \"\"\"\n",
    "    Limpia y preprocesa los datos, aplicando sobremuestreo con SMOTE para equilibrar las clases.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame con los datos a limpiar.\n",
    "    \n",
    "    Returns:\n",
    "        df_scaled (pd.DataFrame): DataFrame con los datos limpios, normalizados y clases equilibradas.\n",
    "    \"\"\"\n",
    "    # Copia de los datos originales para preservarlos\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Aplicar sobremuestreo con SMOTE para equilibrar las clases\n",
    "    X = df.drop(columns=['Potability'])  # Ajusta 'Potability' al nombre de tu columna objetivo\n",
    "    y = df['Potability']                  # Ajusta 'Potability' al nombre de tu columna objetivo\n",
    "    \n",
    "    smote = SMOTE()\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    # Reconstruir el DataFrame después del sobremuestreo\n",
    "    df_resampled = pd.concat([pd.DataFrame(X_resampled, columns=X.columns),\n",
    "                              pd.Series(y_resampled, name='Potability')], axis=1)\n",
    "    \n",
    "    # Imputación de valores faltantes\n",
    "    imputer = IterativeImputer()\n",
    "    df_imputed = pd.DataFrame(imputer.fit_transform(df_resampled), columns=df_resampled.columns)\n",
    "\n",
    "    # Detección y eliminación de valores atípicos\n",
    "    clf = IsolationForest(random_state=0)\n",
    "    outliers = clf.fit_predict(df_imputed.select_dtypes(include=['float64', 'int64']))\n",
    "    df_cleaned = df_imputed[outliers == 1]\n",
    "\n",
    "    # Normalización de los datos\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df_cleaned.select_dtypes(include=['float64', 'int64'])), \n",
    "                             columns=df_cleaned.select_dtypes(include=['float64', 'int64']).columns)\n",
    "    \n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserción de datos en imagen \n",
    "def data_to_image(data):\n",
    "    \"\"\"\n",
    "    Convierte datos en imágenes.\n",
    "    \n",
    "    Args:\n",
    "        data (list): Lista de datos.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Matriz numpy que representa las imágenes.\n",
    "    \"\"\"\n",
    "    data_images = []\n",
    "    font = ImageFont.truetype(\"arial.ttf\", size=20)\n",
    "    for dat in data:\n",
    "        background = np.array([[0 for _ in range(255)] for _ in range(255)], dtype='uint8')\n",
    "        image = Image.fromarray(background)\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        draw.text((40, 190), str(dat[0]), fill='white', font=font)\n",
    "        draw.text((120, 190), str(dat[1]), fill='white', font=font)\n",
    "        draw.text((190, 190), str(dat[2]), fill='white', font=font)\n",
    "        \n",
    "        draw.text((40, 120), str(dat[3]), fill='white', font=font)\n",
    "        draw.text((120, 120), str(dat[4]), fill='white', font=font)\n",
    "        draw.text((190, 120), str(dat[5]), fill='white', font=font)\n",
    "        \n",
    "        draw.text((40, 40), str(dat[6]), fill='white', font=font)\n",
    "        draw.text((110, 40), str(dat[7]), fill='white', font=font)\n",
    "        draw.text((190, 40), str(dat[8]), fill='white', font=font)\n",
    "        \n",
    "        rgb = [np.array(image, dtype='uint8') for _ in range(3)]\n",
    "        data_images.append(rgb)\n",
    "    return np.array(data_images) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barra de grises\n",
    "def data_to_heatmap_bw(data):\n",
    "    data_heatmaps = []\n",
    "\n",
    "    # Normalizar min-max\n",
    "    min_values = np.min(data, axis=0)\n",
    "    max_values = np.max(data, axis=0)\n",
    "    min_max_normalized = (data - min_values) / (max_values - min_values)\n",
    "\n",
    "    # Crear un colormap personalizado en escala de grises\n",
    "    cmap = sns.color_palette(\"Greys\", as_cmap=True)\n",
    "\n",
    "    for row in min_max_normalized:\n",
    "        # Obtener el número de características\n",
    "        num_features = len(row)\n",
    "\n",
    "        # Crear un mapa de calor utilizando Seaborn\n",
    "        ax = sns.heatmap(data=np.expand_dims(row, axis=0), cmap=cmap, cbar=False, square=True)\n",
    "        ax.set_xticks(np.arange(num_features) + 0.5, minor=False)\n",
    "        ax.set_xticklabels(np.arange(1, num_features + 1), minor=False)\n",
    "        ax.set_yticks([])  # Desactivar las etiquetas de los ejes y\n",
    "        heatmap_image = np.array(ax.get_figure().canvas.renderer.buffer_rgba())\n",
    "\n",
    "        # Ajustar el orden de los grupos de colores\n",
    "        if len(data_heatmaps) % 2 == 1:\n",
    "            heatmap_image = heatmap_image[:, ::-1, :]\n",
    "\n",
    "        data_heatmaps.append(heatmap_image)\n",
    "\n",
    "    return np.array(data_heatmaps) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversión de datos a imágenes en escala de grises con 3 canales (En cuadrilla)\n",
    "def gray_gang_gg(data):\n",
    "    cell_size = 85\n",
    "    rgb_images = []\n",
    "    for row in data:\n",
    "        image = np.zeros((255, 255, 3), dtype=np.uint8)\n",
    "        for i, value in enumerate(row):\n",
    "            x = (i % 3) * cell_size\n",
    "            y = (i // 3) * cell_size\n",
    "            grayscale_value = int(value * 255)\n",
    "            image[y:y+cell_size, x:x+cell_size] = [grayscale_value] * 3\n",
    "        rgb_images.append(image)\n",
    "    return np.array(rgb_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Function\n",
    "def data_to_image_pca(data):\n",
    "    data_images = []\n",
    "    num_components = 3  # Number of principal components\n",
    "    pca = PCA(n_components=num_components)\n",
    "    data_pca = pca.fit_transform(data)\n",
    "\n",
    "    for dat in data_pca:\n",
    "        scaled_dat = ((dat - dat.min()) / (dat.max() - dat.min())) * 255\n",
    "        image = Image.new(\"RGB\", (255, 255))\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        for i in range(len(scaled_dat)):\n",
    "            x = scaled_dat[i]\n",
    "            y = 150  # Adjust this vertical position as needed\n",
    "            draw.rectangle([x-1, y-1, x+1, y+1], fill='white')\n",
    "        data_images.append(np.array(image))\n",
    "    \n",
    "    return np.array(data_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **T0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Disabling cudnn for deterministic results\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "# Selecting the device for training (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Loading the autoreload extension to automatically reload modules\n",
    "%load_ext autoreload\n",
    "\n",
    "# Setting autoreload to automatically reload all modules\n",
    "%autoreload 2\n",
    "\n",
    "# Setting matplotlib to display plots inline in the Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting the figure format for inline plotting to 'retina' for better quality\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfate</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>4.668102</td>\n",
       "      <td>193.681736</td>\n",
       "      <td>47580.99160</td>\n",
       "      <td>7.166639</td>\n",
       "      <td>359.948574</td>\n",
       "      <td>526.424171</td>\n",
       "      <td>13.894419</td>\n",
       "      <td>66.687695</td>\n",
       "      <td>4.435821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3272</th>\n",
       "      <td>7.808856</td>\n",
       "      <td>193.553212</td>\n",
       "      <td>17329.80216</td>\n",
       "      <td>8.061362</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>392.449580</td>\n",
       "      <td>19.903225</td>\n",
       "      <td>66.396293</td>\n",
       "      <td>2.798243</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3273</th>\n",
       "      <td>9.419510</td>\n",
       "      <td>175.762646</td>\n",
       "      <td>33155.57822</td>\n",
       "      <td>7.350233</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>432.044783</td>\n",
       "      <td>11.039070</td>\n",
       "      <td>69.845400</td>\n",
       "      <td>3.298875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>5.126763</td>\n",
       "      <td>230.603758</td>\n",
       "      <td>11983.86938</td>\n",
       "      <td>6.303357</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>402.883113</td>\n",
       "      <td>11.168946</td>\n",
       "      <td>77.488213</td>\n",
       "      <td>4.708658</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3275</th>\n",
       "      <td>7.874671</td>\n",
       "      <td>195.102299</td>\n",
       "      <td>17404.17706</td>\n",
       "      <td>7.509306</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>327.459761</td>\n",
       "      <td>16.140368</td>\n",
       "      <td>78.698446</td>\n",
       "      <td>2.309149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ph    Hardness       Solids  Chloramines     Sulfate  \\\n",
       "3271  4.668102  193.681736  47580.99160     7.166639  359.948574   \n",
       "3272  7.808856  193.553212  17329.80216     8.061362  333.775777   \n",
       "3273  9.419510  175.762646  33155.57822     7.350233  333.775777   \n",
       "3274  5.126763  230.603758  11983.86938     6.303357  333.775777   \n",
       "3275  7.874671  195.102299  17404.17706     7.509306  333.775777   \n",
       "\n",
       "      Conductivity  Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
       "3271    526.424171       13.894419        66.687695   4.435821           1  \n",
       "3272    392.449580       19.903225        66.396293   2.798243           1  \n",
       "3273    432.044783       11.039070        69.845400   3.298875           1  \n",
       "3274    402.883113       11.168946        77.488213   4.708658           1  \n",
       "3275    327.459761       16.140368        78.698446   2.309149           1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/drinking_water_potability.csv')\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfate</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.466496</td>\n",
       "      <td>0.538858</td>\n",
       "      <td>0.396489</td>\n",
       "      <td>0.515171</td>\n",
       "      <td>0.607263</td>\n",
       "      <td>0.669439</td>\n",
       "      <td>0.251081</td>\n",
       "      <td>0.699753</td>\n",
       "      <td>0.306355</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.188638</td>\n",
       "      <td>0.229369</td>\n",
       "      <td>0.353777</td>\n",
       "      <td>0.451496</td>\n",
       "      <td>0.462049</td>\n",
       "      <td>0.719411</td>\n",
       "      <td>0.451691</td>\n",
       "      <td>0.450999</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.550590</td>\n",
       "      <td>0.618194</td>\n",
       "      <td>0.379063</td>\n",
       "      <td>0.704357</td>\n",
       "      <td>0.462049</td>\n",
       "      <td>0.414652</td>\n",
       "      <td>0.522262</td>\n",
       "      <td>0.532866</td>\n",
       "      <td>0.325143</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.568563</td>\n",
       "      <td>0.577747</td>\n",
       "      <td>0.420740</td>\n",
       "      <td>0.587863</td>\n",
       "      <td>0.558649</td>\n",
       "      <td>0.317880</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.808065</td>\n",
       "      <td>0.643585</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.632600</td>\n",
       "      <td>0.441300</td>\n",
       "      <td>0.340910</td>\n",
       "      <td>0.443007</td>\n",
       "      <td>0.363236</td>\n",
       "      <td>0.379337</td>\n",
       "      <td>0.300332</td>\n",
       "      <td>0.253606</td>\n",
       "      <td>0.531482</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3539</th>\n",
       "      <td>0.487644</td>\n",
       "      <td>0.424335</td>\n",
       "      <td>0.391671</td>\n",
       "      <td>0.524607</td>\n",
       "      <td>0.551578</td>\n",
       "      <td>0.809093</td>\n",
       "      <td>0.489516</td>\n",
       "      <td>0.649463</td>\n",
       "      <td>0.613577</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3540</th>\n",
       "      <td>0.517059</td>\n",
       "      <td>0.213649</td>\n",
       "      <td>0.635107</td>\n",
       "      <td>0.576354</td>\n",
       "      <td>0.407959</td>\n",
       "      <td>0.512183</td>\n",
       "      <td>0.549762</td>\n",
       "      <td>0.423245</td>\n",
       "      <td>0.692120</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3541</th>\n",
       "      <td>0.335455</td>\n",
       "      <td>0.627784</td>\n",
       "      <td>0.222100</td>\n",
       "      <td>0.448721</td>\n",
       "      <td>0.462049</td>\n",
       "      <td>0.376690</td>\n",
       "      <td>0.278169</td>\n",
       "      <td>0.600486</td>\n",
       "      <td>0.695354</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3542</th>\n",
       "      <td>0.470389</td>\n",
       "      <td>0.487349</td>\n",
       "      <td>0.444984</td>\n",
       "      <td>0.514232</td>\n",
       "      <td>0.494689</td>\n",
       "      <td>0.307259</td>\n",
       "      <td>0.350640</td>\n",
       "      <td>0.495031</td>\n",
       "      <td>0.423521</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3543</th>\n",
       "      <td>0.489846</td>\n",
       "      <td>0.509890</td>\n",
       "      <td>0.630193</td>\n",
       "      <td>0.607914</td>\n",
       "      <td>0.368411</td>\n",
       "      <td>0.544442</td>\n",
       "      <td>0.381083</td>\n",
       "      <td>0.428285</td>\n",
       "      <td>0.515431</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3544 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ph  Hardness    Solids  Chloramines   Sulfate  Conductivity  \\\n",
       "0     0.466496  0.538858  0.396489     0.515171  0.607263      0.669439   \n",
       "1     0.188638  0.229369  0.353777     0.451496  0.462049      0.719411   \n",
       "2     0.550590  0.618194  0.379063     0.704357  0.462049      0.414652   \n",
       "3     0.568563  0.577747  0.420740     0.587863  0.558649      0.317880   \n",
       "4     0.632600  0.441300  0.340910     0.443007  0.363236      0.379337   \n",
       "...        ...       ...       ...          ...       ...           ...   \n",
       "3539  0.487644  0.424335  0.391671     0.524607  0.551578      0.809093   \n",
       "3540  0.517059  0.213649  0.635107     0.576354  0.407959      0.512183   \n",
       "3541  0.335455  0.627784  0.222100     0.448721  0.462049      0.376690   \n",
       "3542  0.470389  0.487349  0.444984     0.514232  0.494689      0.307259   \n",
       "3543  0.489846  0.509890  0.630193     0.607914  0.368411      0.544442   \n",
       "\n",
       "      Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
       "0           0.251081         0.699753   0.306355         0.0  \n",
       "1           0.451691         0.450999   0.617647         0.0  \n",
       "2           0.522262         0.532866   0.325143         0.0  \n",
       "3           0.587787         0.808065   0.643585         0.0  \n",
       "4           0.300332         0.253606   0.531482         0.0  \n",
       "...              ...              ...        ...         ...  \n",
       "3539        0.489516         0.649463   0.613577         1.0  \n",
       "3540        0.549762         0.423245   0.692120         1.0  \n",
       "3541        0.278169         0.600486   0.695354         1.0  \n",
       "3542        0.350640         0.495031   0.423521         1.0  \n",
       "3543        0.381083         0.428285   0.515431         1.0  \n",
       "\n",
       "[3544 rows x 10 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned = clean_data(df)\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABH4AAANdCAYAAADiKrsPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAB7CAAAewgFu0HU+AABoaElEQVR4nOz9e7xXZZ03/r8+wOawQQE1EcEDdicobDPpdnQmMQHLb2qaeUjII2lO6j1mv1JTmm5PWeboqKilkMZ4M+lojZTmgUBjxsjSFJFD4NAtaEhyEDzA3uzP7w/u/Rl2bGBvju61n8/Hg8cs1rqua73XjnkseXGt6yqVy+VyAAAAACicdju6AAAAAAC2DcEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAF1WFHF8AHX319fd57770kSYcOHVIqlXZwRQAAAFA85XI5dXV1SZIuXbqkXbstn68j+GGT3nvvvcyaNWtHlwEAAABtxoABA9K1a9ctHsenXgAAAAAFZcYPm9Shw3//MRkwYECqqqp2YDUAAABQTLW1tZUvbtb9u/iWEPywSeuu6VNVVZWOHTvuwGoAAACg+LbW+ro+9QIAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABdVhRxfwQXDttddm/Pjxufbaa3PKKaesd33BggUZNmxYi8Y89NBDM378+Ebnvv3tb2fChAmb7Dt69Oh88YtfXO98bW1tJkyYkEceeSTz5s1LuVxOnz59Mnz48Jxzzjnp0aNHi2oEAAAAiq3NBz+TJk3K/fffv9XH7dq163rnXnnllc0eb9WqVRk1alSee+65Rufnzp2buXPn5uGHH87YsWOz//77b/Y92oo19fX5y9vv7egyAPgA223nLmnfzsRoAKD1a9PBz+TJk3PJJZekvr5+o+369OmT559/fqNtyuVy/v7v/z6//e1v07Nnz1x11VWNrq9ZsyazZ89Oklx99dU57rjjNjhWx44d1zt32WWX5bnnnktVVVUuuuiiHHfccenYsWOmTJmSG2+8MW+++WYuuOCC/PznP091dfVGa23r/vL2e/nMdQ/t6DIA+AB79MrPp1eP9f8Rhx3HP9wAsCn+4aZpbTL4qa+vz5gxY3LHHXdsMvRJklKp1OQMnnXdfffd+e1vf5skueGGG9K3b99G1+fOnZv3338/STJ48OBNjreul156KY899liS5Morr8zpp59euXbqqafmgAMOyOmnn56FCxfmvvvuy9///d83e2wAgNbAP9wAsCn+4aZpbS4Kmzp1ak488cTcfvvtqa+vz8CBA7d4zBkzZuSWW25JsjaI+eQnP7lem4bPvKqrq7Pffvu1aPxx48YlWTvz6NRTT13vek1NTY4//vgkyYMPPtiisQEAAIDianPBz6hRozJ79uxUVVXl4osvrgQ2W+Kaa65JXV1ddtttt3z9619vss2MGTOSJIMGDUq7Fkw9K5fLmTp1apLkyCOPTPv27Zts17D49MKFCzNz5syWlA8AAAAUVJv71KtUKmXYsGG59NJL8+EPfzgLFizYovEeeeSRvPDCC0mSSy65JDvvvHOT7RqCnwEDBuSBBx7II488kpkzZ6a2tjZ9+vTJsGHDMmrUqPTs2bNRvwULFmTFihVJstHZSQcccEDl+OWXX270ewAAAKBtanPBz2OPPZZ+/fptlbFWr16dm266KUmy33775fOf/3yT7err6zNr1qwkyYQJE1JbW9vo+quvvppXX301Dz30UO68884cfPDBlWsLFy6sHPfp02eDtfTq1Svt27fPmjVrGvXZ2v669takqqoqpVJpR5cBQCtSLpdb9buvCLy/AWip1vz+3hZ1t7ngZ2uFPkny85//PH/+85+TJF/60pc2+AnXf/3Xf+Xdd99NktTV1WXEiBE5+eSTs+eee2bx4sWZOHFixo0blyVLluT888/PQw89lL322itJsnTp0so43bt332AtHTp0SJcuXbJy5cq8/fbbW+sR19MQYLVGNTU1Te6YBgAbUltbm+nTp+/oMto0728AWsr7u7E2t8bP1vSjH/0oSdK7d+989rOf3WC7N998M71790779u1z44035h//8R8zcODA9OzZM/vvv3++9rWv5eabb06SLF++PDfeeGOl76pVqyrHnTp12mg9nTt3TpLK7mEAAABA29bmZvxsLf/5n/+ZOXPmJEnOPvvsVFVVbbDt4YcfnilTpqSuri4dOjT9I//Upz6Vo446KpMnT86TTz6Z5cuXp3v37o0Wc97UNOdyuZwkLVo8uqUGDBiw0Wf9IGutdQOw41RVVaWmpmZHl9GmeX8D0FKt+f1dW1u71b+0EfxspokTJyZZ+4nVxmb7rGtDoU+DYcOGZfLkyamvr8/LL7+cv/u7v0t1dXXl+rqzf5rScH1TM4O2RFVVlenWALQZpVLJew8AWhnv78Z86rUZ6urq8qtf/SpJcthhh2WXXXbZKuP27t27crxkyZIkyU477VQ517C714Zqeu+995JkvZ3BAAAAgLZJ8LMZnnvuuSxbtixJ8pnPfKbZ/Ro+xdqQdVfv7tKlS5Jk3333rZx7/fXXN9h30aJFWbNmTZJkzz33bHZNAAAAQHEJfjbDM888k2Tt9LFhw4Ztsv3Xvva1HHbYYTnmmGM22m7u3LmV44bdx3bffffKDJ6ZM2dusO8rr7xSOT7ggAM2WRMAAABQfIKfzfDCCy8kWRvO9OjRY5Ptu3XrlqVLl2b+/PmZP39+k23K5XJ+8YtfJEn69OmT/fbbr3LtyCOPTJJMmTIl9fX1TfafNGlSkuRDH/pQBgwY0NxHAQAAAApM8NNC9fX1lRW2Dz744Gb1WXfx52uuuabJNnfffXdlRs+oUaMa7eB14oknJknmz5+fCRMmrNf3pZdeqiw2fdZZZ21y9y8AAACgbRD8tNDChQsriyh/5CMfaVafwYMH59hjj02STJ06NWeffXaee+65LFmyJLNmzcro0aNz0003JUkOPfTQnH766Y36H3744Rk6dGiS5LrrrsvNN9+c1157LYsXL86DDz6YL33pS6mrq0vfvn3X6wsAAAC0XbZzb6E33nijcrzHHns0u9/111+fd999N5MnT86zzz6bZ599dr02f/u3f5vbbrst7dqtn8fdcMMNGTVqVKZPn5677rord911V6Pru+22W8aOHZtu3bq14GkAAACAIhP8tNDbb79dOW5J8NO5c+fceeedeeKJJ/LQQw9l+vTpWbFiRbp3754BAwbkc5/7XI499tgNfqbVvXv3TJgwIRMmTMjEiRMzb968rF69On369MlRRx2V8847L7vuuusWPx8AAABQHG0++Onbt29mz57d7PbDhw9vUft1lUqlfPrTn86nP/3pzepfVVWVM888M2eeeeZm9QcAAADaFmv8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFAddnQBHwTXXnttxo8fn2uvvTannHLKBtstWbIkhx9++CbH69GjR6ZNm9bktdmzZ+eee+7JtGnTsmTJkvTo0SODBg3KiBEjMmTIkI2OW1tbmwkTJuSRRx7JvHnzUi6X06dPnwwfPjznnHNOevToscnaAAAAgLajzQc/kyZNyv3339+sti+//PIW3eupp57KJZdcktra2sq5xYsXZ/LkyZk8eXLOOOOMXHXVVU32XbVqVUaNGpXnnnuu0fm5c+dm7ty5efjhhzN27Njsv//+W1QjAAAAUBxtOviZPHlyLrnkktTX1zer/SuvvJIk6dOnTyZOnLjBdqVSab1zM2bMyKWXXpra2trU1NTkG9/4Rj7ykY9kwYIFufPOOzNp0qSMHz8+/fr1y8iRI9frf9lll+W5555LVVVVLrroohx33HHp2LFjpkyZkhtvvDFvvvlmLrjggvz85z9PdXV1M38CAAAAQJG1yTV+6uvrc9ttt+UrX/lKVq9e3ex+M2bMSJLU1NSka9euG/zVVPByyy23ZNWqVdl7771z33335dBDD03Pnj1TU1OTMWPG5Oijj06S3HrrrVm5cmWjvi+99FIee+yxJMmVV16ZCy64IH379s3uu++eU089NePGjUtVVVUWLlyY++67b3N/LAAAAEDBtLngZ+rUqTnxxBNz++23p76+PgMHDmx233WDn5aYN29ennnmmSTJ+eefn65duza6XiqVcvnll6dUKmXZsmV5/PHHG10fN25ckrUzjU499dT1xq+pqcnxxx+fJHnwwQdbVBsAAABQXG0u+Bk1alRmz56dqqqqXHzxxbnlllua1W/58uVZuHBhkpYHPw2hT6lUytChQ5ts07dv3/Tv3z/J2nWHGpTL5UydOjVJcuSRR6Z9+/ZN9h82bFiSZOHChZk5c2aL6gMAAACKqc0FP6VSKcOHD8+///u/56KLLkq7ds37ETTM9imVSunUqVO+9a1vZejQoRk0aFAOO+ywnH/++ZkyZUqTfRuCmD322CO77rrrBu9x4IEHNrpXkixYsCArVqxIko3OTjrggAMqx1u6CDUAAABQDG1ucefHHnss/fr1a3G/hjCmXbt2GTlyZOrq6irXli5dmqeffjpPP/10TjrppFxzzTXp0OG/f7Svv/56krWfam3MnnvumSRZtGhRamtrK+v2NNhY/169eqV9+/ZZs2ZNoz5b27o7krU2VVVVTS68DQAbUi6XW/W7rwi8vwFoqdb8/t4Wdbe54GdzQp/kv4OfNWvWpF+/frnooosyePDgVFVV5Q9/+ENuu+22zJo1Kw8//HC6deuWK6+8stJ36dKlSZKdd955o/fYaaedkqz9Q7pixYrssssulb5J0r179w327dChQ7p06ZKVK1fm7bff3qxnbI5Zs2Zts7G3tZqamnTs2HFHlwFAK1JbW5vp06fv6DLaNO9vAFrK+7uxNvep1+ZavXp1unbtmoEDB+ahhx7Kcccdl969e2e33XbL8OHD85Of/CQHH3xwkmT8+PGZPXt2pe+qVauSJJ07d97oPTp16rRen4b/+9fXm9Iw/vvvv9/8BwMAAAAKq83N+Nlcd9xxR5Kkrq6u0WdcDTp37pzRo0fn85//fMrlch5++OFcccUVSVJZkHlT05TL5XLluGHtoXUXc25u/+auW7Q5BgwYkKqqqm02/rbUWusGYMepqqpq8aYObF3e3wC0VGt+f9fW1m71L20EPy3UVOjTYNCgQenVq1cWLVqUF198sXK+uro6yaZn4qxevbpy3DCluaFv0nj2T1Marm9qZtCWqKqqMt0agDajVCp57wFAK+P93ZhPvbayhgWa112bp1u3bklS2Z1rQxrW5mnXrl1lPZ+GdX821b+uri7vvfdekqRnz56bUTkAAABQNIKfFlr3c6ymNMza6dKlS+Vcw4LSb7zxxkb7Nlzv3bt35XOtfffdt3K9YXewpixatChr1qxJ8t/hEwAAANC2CX6a4YUXXsjQoUPz0Y9+NI888sgG261Zsybz589P0jiw6d+/f5Jk4cKFG91xq2HnsAEDBlTO7b777pUZPDNnztxg31deeaVyfMABB2z4YQAAAIA2Q/DTDH369Mnrr7+e999/P88888wG2/3qV7/KO++8kyQZMmRI5XzDcX19faZMmdJk39deey1z5sxJkhxxxBGNrh155JFJkilTpqS+vr7J/pMmTUqSfOhDH2oUHAEAAABtl+CnGXbfffccfvjhSZJHH300v//979drs3jx4nznO99Jkuyxxx459thjK9f22muvDB48OEkyZsyY9dbqKZfLueGGG1Iul9OzZ8+ccMIJja6feOKJSZL58+dnwoQJ6937pZdeysSJE5MkZ5111iZ3/wIAAADaBsFPM1122WXp1KlT6uvrc9555+Xee+/N/Pnzs3jx4kycODGnnXZaFi5cmA4dOuS6665bb2etK664Iu3atcv8+fMzYsSITJ06NUuWLMmMGTNy4YUX5qmnnkqSXHzxxY128kqSww8/PEOHDk2SXHfddbn55pvz2muvZfHixXnwwQfzpS99KXV1denbt29OP/307fMDAQAAAD7wbOfeTAMGDMhtt92WSy+9NCtXrsx3vvOdygyfBtXV1bn++uvziU98Yr3+NTU1ue666zJ69OjMmTMno0aNWq/NOeeck5EjRzZ5/xtuuCGjRo3K9OnTc9ddd+Wuu+5qdH233XbL2LFjKzuIAQAAAAh+WuDII4/Mo48+mnvvvTe//vWvs2DBgiRrd+EaMmRIzjrrrI3uqHXSSSdl4MCBGTt2bKZNm5a33nor1dXVGTRoUEaMGJHhw4dvsG/37t0zYcKETJgwIRMnTsy8efOyevXq9OnTJ0cddVTOO++87Lrrrlv9mQEAAIDWq1Te1P7ktHmrV6/O9OnTk6ydudSxY8cdXNGWWbTsnXzmuod2dBkAfIA9euXn06tH1x1dBuvw/gZgU4rw/t4Wf/+2xg8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKA67OgCPgiuvfbajB8/Ptdee21OOeWUjbZdsmRJ/uVf/iVTpkzJn/70p6xatSo9e/bMRz/60Zxyyik58sgjN9j329/+diZMmLDJekaPHp0vfvGL652vra3NhAkT8sgjj2TevHkpl8vp06dPhg8fnnPOOSc9evTY5NgAAABA29Hmg59Jkybl/vvvb1bb3/3ud7nooouydOnSRufffPPNPPnkk3nyySfzuc99Ltddd13at2+/Xv9XXnlls+tctWpVRo0aleeee67R+blz52bu3Ll5+OGHM3bs2Oy///6bfQ8AAACgWNp08DN58uRccsklqa+v32TbRYsW5YILLsiKFSvSo0eP/MM//EOGDBmSjh075o9//GNuv/32PP/88/npT3+a3XbbLf+//9//r1H/NWvWZPbs2UmSq6++Oscdd9wG79WxY8f1zl122WV57rnnUlVVlYsuuijHHXdcOnbsmClTpuTGG2/Mm2++mQsuuCA///nPU11d3cKfBAAAAFBEbXKNn/r6+tx22235yle+ktWrVzerz5133pkVK1akU6dO+fGPf5wRI0akb9++2X333fN3f/d3+T//5/9k6NChSZJ77703b775ZqP+c+fOzfvvv58kGTx4cLp27brBX1VVVY36vvTSS3nssceSJFdeeWUuuOCCyr1PPfXUjBs3LlVVVVm4cGHuu+++Lf3xAAAAAAXR5oKfqVOn5sQTT8ztt9+e+vr6DBw4sFn9Hn/88STJsccem/79+693vVQq5R/+4R+SrF2L5z/+4z8aXW/4zKu6ujr77bdfi2oeN25ckqRPnz459dRT17teU1OT448/Pkny4IMPtmhsAAAAoLjaXPAzatSozJ49O1VVVbn44otzyy23bLJPw5o+pVIpBx100Abb7bPPPpXjv57xM2PGjCTJoEGD0q5d83/s5XI5U6dOTZIceeSRTa4dlCTDhg1LkixcuDAzZ85s9vgAAABAcbW5NX5KpVKGDRuWSy+9NB/+8IezYMGCTfbp2bNnnn322axevTrlcnmD7f70pz9VjnfeeedG1xqCnwEDBuSBBx7II488kpkzZ6a2tjZ9+vTJsGHDMmrUqPTs2bNRvwULFmTFihVJstHZSQcccEDl+OWXX270ewAAAKBtanPBz2OPPZZ+/fptVt+mFl1e17pbtR9yyCGV4/r6+syaNavSpra2tlG/V199Na+++moeeuih3HnnnTn44IMr1xYuXFg57tOnzwbv3atXr7Rv3z5r1qxp1Gdr++vaW5OqqqqUSqUdXQYArUi5XG7V774i8P4GoKVa8/t7W9Td5oKfzQ19NuV3v/tdZX2dwYMHN1oH6L/+67/y7rvvJknq6uoyYsSInHzyydlzzz2zePHiTJw4MePGjcuSJUty/vnn56GHHspee+2VJI22ju/evfsG79+hQ4d06dIlK1euzNtvv70tHjFJKgFWa1RTU7PJ8A4A1lVbW5vp06fv6DLaNO9vAFrK+7uxNrfGz7Ywb968XHzxxVmzZk06deqU0aNHN7r+5ptvpnfv3mnfvn1uvPHG/OM//mMGDhyYnj17Zv/998/Xvva13HzzzUmS5cuX58Ybb6z0XbVqVeW4U6dOG62jc+fOSVLZPQwAAABo29rcjJ+tbc6cOTn33HOzZMmSJMn//t//e731dQ4//PBMmTIldXV16dCh6R/5pz71qRx11FGZPHlynnzyySxfvjzdu3dvtJjzpqY5N6w/1JLFo1tqwIAB620331q01roB2HGqqqpSU1Ozo8to07y/AWip1vz+rq2t3epf2gh+tsBzzz2XCy+8MMuXL0+SfPOb38znPve5DbbfUOjTYNiwYZk8eXLq6+vz8ssv5+/+7u9SXV1dub7u7J+mNFzf1MygLVFVVWW6NQBtRqlU8t4DgFbG+7sxn3ptpp/+9Kc555xzsnz58rRr1y7XXHNNzjrrrC0as3fv3pXjhhlEO+20U+Vcw+5eTamrq8t7772XJOvtDAYAAAC0TWb8bIbbbrstt99+e5KkS5cuuemmmzJs2LBN9iuXyxv9XGvd1bu7dOmSJNl3330r515//fUN9l20aFHWrFmTJNlzzz03WQsAAABQfGb8tEC5XM63vvWtSuiz22675cc//vEmQ5+vfe1rOeyww3LMMcdstN3cuXMrxw27j+2+++6VGTwzZ87cYN9XXnmlcvzXawwBAAAAbZPgpwVuuOGG/OQnP0mydibOv/7rv+aggw7aZL9u3bpl6dKlmT9/fubPn99km3K5nF/84hdJkj59+mS//farXDvyyCOTJFOmTEl9fX2T/SdNmpQk+dCHPpQBAwY0+5kAAACA4hL8NNPjjz+ee++9N8na0Of+++/PXnvt1ay+n/3sZyvH11xzTZNt7r777sqMnlGjRjX6JOzEE09MksyfPz8TJkxYr+9LL72UiRMnJknOOuusTe7+BQAAALQNgp9mWL16da677rokSceOHfPd7343Xbp0yTvvvLPBX6tXr670Hzx4cI499tgkydSpU3P22Wfnueeey5IlSzJr1qyMHj06N910U5Lk0EMPzemnn97o/ocffniGDh2aJLnuuuty880357XXXsvixYvz4IMP5ktf+lLq6urSt2/f9foCAAAAbZfFnZvhl7/8ZRYtWpRkbQh02mmnbbLPRRddlIsvvrjy++uvvz7vvvtuJk+enGeffTbPPvvsen3+9m//NrfddlvatVs/j7vhhhsyatSoTJ8+PXfddVfuuuuuRtd32223jB07Nt26dWvp4wEAAAAFJfhphhdffHGLx+jcuXPuvPPOPPHEE3nooYcyffr0rFixIt27d8+AAQPyuc99Lscee+wGP9Pq3r17JkyYkAkTJmTixImZN29eVq9enT59+uSoo47Keeedl1133XWL6wQAAACKo1Qul8s7ugg+2FavXp3p06cnSWpqatKxY8cdXNGWWbTsnXzmuod2dBkAfIA9euXn06tH1x1dBuvw/gZgU4rw/t4Wf/+2xg8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKA67OgCPgiuvfbajB8/Ptdee21OOeWUjbatra3NhAkT8sgjj2TevHkpl8vp06dPhg8fnnPOOSc9evTYaP/Zs2fnnnvuybRp07JkyZL06NEjgwYNyogRIzJkyJBtem8AAACgbWnzwc+kSZNy//33N6vtqlWrMmrUqDz33HONzs+dOzdz587Nww8/nLFjx2b//fdvsv9TTz2VSy65JLW1tZVzixcvzuTJkzN58uScccYZueqqq7bJvQEAAIC2p01/6jV58uRccsklqa+vb1b7yy67LM8991yqqqry1a9+NZMmTcqvf/3rXHPNNdl5553z5ptv5oILLsi77767Xt8ZM2bk0ksvTW1tbWpqajJ+/Pj85je/yb/9279l2LBhSZLx48dvMITaknsDAAAAbVObDH7q6+tz22235Stf+UpWr17drD4vvfRSHnvssSTJlVdemQsuuCB9+/bN7rvvnlNPPTXjxo1LVVVVFi5cmPvuu2+9/rfccktWrVqVvffeO/fdd18OPfTQ9OzZMzU1NRkzZkyOPvroJMmtt96alStXbtV7AwAAAG1Tmwt+pk6dmhNPPDG333576uvrM3DgwGb1GzduXJKkT58+OfXUU9e7XlNTk+OPPz5J8uCDDza6Nm/evDzzzDNJkvPPPz9du3ZtdL1UKuXyyy9PqVTKsmXL8vjjj2+1ewMAAABtV5sLfkaNGpXZs2enqqoqF198cW655ZZN9imXy5k6dWqS5Mgjj0z79u2bbNfwydbChQszc+bMyvmG0KdUKmXo0KFN9u3bt2/69++fZO26Q1vr3gAAAEDb1eaCn1KplOHDh+ff//3fc9FFF6Vdu03/CBYsWJAVK1YkyUZnCB1wwAGV45dffrly3BDE7LHHHtl111032P/AAw9MsnY9oK11bwAAAKDt2i67ev3sZz9Lknz2s59tVtDy7rvvZty4cXn77bfzzW9+c6vW8thjj6Vfv34t6rNw4cLKcZ8+fTbYrlevXmnfvn3WrFnTqM/rr7++yb5JsueeeyZJFi1alNra2sq6PVty761t3R3JWpuqqqqUSqUdXQYArUi5XG7V774i8P4GoKVa8/t7W9S9XYKfyy+/PO3atcunP/3pdOnSZZPta2trc/vtt2ennXba6sFPS0OfJFm6dGnluHv37hts16FDh3Tp0iUrV67M22+/vV7/nXfeeaP32WmnnZKs/UO6YsWK7LLLLlt8761t1qxZ22zsba2mpiYdO3bc0WUA0IrU1tZm+vTpO7qMNs37G4CW8v5ubLt96lUul5v1rzV1dXWVxY3r6uq2dVnNsmrVqspxp06dNtq2c+fOSZL3339/vf4N1zZk3bEb+mzpvQEAAIC2a6vO+Kmvr8+pp57aaI2aJJXA52Mf+1izxyqVSpXFjne0dRdU3lR4VS6Xk6TRJ20N/Zvbd93+W3rvrW3AgAGpqqraZuNvS621bgB2nKqqqtTU1OzoMto0728AWqo1v79ra2u3+pc2WzX4adeuXa6++uqcfPLJqa+v36Kxqqur87WvfW0rVbZlqqurK8frzsBpSsP1dWfnNPTf1Eyc1atXV44bpjRv6b23tqqqKtOtAWgzSqWS9x4AtDLe341t9TV+DjzwwPzwhz/M4sWLK+euuOKKlEqlfPvb397oD79UKqV9+/bp0aNHBg0alJ49e27t8jZLw9o7SSo7bDWlrq4u7733XpI0qr1bt26b7JuksjZPu3btKuv5bOm9AQAAgLZrmyzu/IlPfKLR76+44ooka3f1as7izh80++67b+W4YYeupixatChr1qxJ8t87dCVrF5T+7W9/mzfeeGOj92m43rt378rnWlt6bwAAAKDt2i6LO//4xz/Offfdt8nFjT+odt9998osmpkzZ26w3SuvvFI5PuCAAyrHDWsVLVy4cKM7bjWsjTRgwICtdm8AAACg7douwc+hhx6aQw89tFm7en1QHXnkkUmSKVOmbHD9okmTJiVJPvShDzUKb4YMGZJk7eLXU6ZMabLva6+9ljlz5iRJjjjiiK12bwAAAKDt2iafem3MK6+8kgULFuS9995rtIvVhpx44onbvqhmOPHEE/Ozn/0s8+fPz4QJEzJy5MhG11966aVMnDgxSXLWWWc1Crn22muvDB48OL///e8zZsyYHHXUUY3W7imXy7nhhhtSLpfTs2fPnHDCCVvt3gAAAEDbtd2Cn2effTajR4/OwoULm92nVCp9YIKfww8/PEOHDs2vfvWrXHfddXnzzTdz8sknp3PnzpkyZUpuvPHG1NXVpW/fvjn99NPX63/FFVfk1FNPzfz58zNixIhcdtllOfDAA/PGG29kzJgxlRk7F198caOdvLbGvQEAAIC2absEP6+++mq+/OUvp7a2tlmzfD6obrjhhowaNSrTp0/PXXfdlbvuuqvR9d122y1jx46t7OK1rpqamlx33XUZPXp05syZk1GjRq3X5pxzzllvNs/WuDcAAADQNm2X4Ofuu+/O6tWr06FDh5xxxhn55Cc/mV133XWjW7t/EHXv3j0TJkzIhAkTMnHixMybNy+rV69Onz59ctRRR+W8887LrrvuusH+J510UgYOHJixY8dm2rRpeeutt1JdXZ1BgwZlxIgRGT58+Da7NwAAAND2bJfg59lnn02pVMpXvvKVfOUrX9ket2y2vn37Zvbs2c1uX1VVlTPPPDNnnnnmZt2vf//++d73vrdZfbf03gAAAEDbsl129XrrrbeSfHAWagYAAABoC7ZL8NOjR48kSefOnbfH7QAAAADIdgp+Pv7xjydJnn/++e1xOwAAAACynYKfc845J+3atcs///M/5913390etwQAAABo87bL4s4HHXRQvv3tb+fb3/52TjjhhIwYMSIHHXRQevbsmaqqqo323WuvvbZHiQAAAACFs12Cn09+8pNJkvbt22fBggXN3tWqVCrllVde2YaVAQAAABTXdgl+/vznP2+P2wAAAACwju0S/HznO9/ZHrcBAAAAYB3bJfj53Oc+tz1uAwAAAMA6tsuuXgAAAABsf4IfAAAAgILaLp96jRw5crP6lUql/Mu//MtWrgYAAACgbdguwc/vf//7lEqllMvlDbYplUqV44Z2654DAAAAoGW2S/DzP//n/9zo9VWrVmXZsmV57bXXUi6X071795xxxhlp37799igPAAAAoJC2S/Azfvz4ZrVbsmRJbrnlljzwwAOZM2dObr311m1cGQAAAEBxfaAWd95ll11y9dVX51Of+lSefPLJ/OxnP9vRJQEAAAC0Wh+o4KfBueeem3K5nH/7t3/b0aUAAAAAtFofyOBnn332SZLMnj17B1cCAAAA0Hp9IIOf1157LUlSV1e3gysBAAAAaL0+cMHPqlWr8v3vfz9J0q9fvx1cDQAAAEDrtV129frnf/7nTbapq6vL0qVL8/TTT+cvf/lLSqVSjj322O1QHQAAAEAxbZfg584770ypVGpW23K5nCQ58MADc8YZZ2zLsgAAAAAKbbsEP8l/Bzob0q5du3Tp0iX77LNPhg0blrPPPjsdO3bcTtUBAAAAFM92CX5mzZq1PW4DAAAAwDo+cIs7AwAAALB1bLdPvf7a8uXL88Ybb+Tdd99NdXV19txzz+y88847qhwAAACAwtmuwU99fX1+8pOfZMKECfnjH/+43vV+/frl1FNPzZlnnpl27UxGAgAAANgS2y34Wb58eS688ML8/ve/3+BCz6+++mq++93v5oknnsiYMWPSs2fP7VUeAAAAQOFst+Dnf/2v/5Xf/e53SZL+/fvnmGOOyYc//OF06dIlK1euzLx58/LEE09kzpw5eeGFF/L1r38999xzz/YqDwAAAKBwtkvw84tf/CLTpk1LqVTKZZddlrPPPrvJdhdddFHuvffefPe7381//Md/ZNKkSRk2bNj2KBEAAACgcLbLQjqPPPJISqVSTjvttA2GPg3OPvvsfOELX0i5XM5DDz20PcoDAAAAKKTtEvzMmDEjSfKFL3yhWe1PO+20JMmsWbO2WU0AAAAARbddgp9ly5YlSfbcc89mtW9o95e//GVblQQAAABQeNsl+Nl5552TJK+//nqz2je022mnnbZZTQAAAABFt12Cn4EDByZJHnzwwWa1f+CBB5IkBxxwwDarCQAAAKDotkvwc/zxx6dcLmfChAn5l3/5l422HT9+fCZMmJBSqZTjjz9+e5QHAAAAUEjbZTv34447LhMmTMgLL7yQ6667Lg8//HA+9alP5cMf/nCqq6vz7rvvZt68eXniiScyc+bMlMvlHHzwwfnsZz+7PcoDAAAAKKTtEvy0a9cuY8aMyXnnnZcZM2bklVdeycyZM9drVy6XkySDBg3KHXfckVKptD3KAwAAACik7RL8JMkuu+ySf/3Xf81PfvKTPPjgg5kzZ04l6EmSUqmUj3zkIznttNNy2mmnpaqqanuVBgAAAFBI2y34SZKqqqocddRROeigg7LPPvtk0aJFWblyZaqrq/P+++/nN7/5TYYMGSL0AQAAANgKtsvizknyzjvv5LLLLsunPvWp3HnnnenevXv233//HHLIIRkwYECef/75/PM//3OOOeaYXH311amrq9tepQEAAAAU0naZ8bNmzZqcd955eeGFF1IulzN//vz12tTV1aV9+/apq6vLhAkT8pe//CW33nrr9igPAAAAoJC2y4yfBx54IM8//3yS5Mwzz8y99967Xpvzzz8/06ZNy6hRo1IqlfLkk0/mscce2x7lAQAAABTSdgl+Jk6cmFKplDPOOCPf/OY306tXrybbde3aNV//+tdz+umnp1wu58EHH9we5QEAAAAU0nYJfubMmZMk+cIXvtCs9qecckqSZMaMGdusJgAAAICi2y7BT21tbZK1W7o3xx577JEkeffdd7dZTQAAAABFt12Cn4Yg549//GOz2jcs/tzcoAgAAACA9W2X4Gfw4MEpl8u54447Ui6XN9n+Bz/4QUqlUgYPHrwdqgMAAAAopu0S/DSs7fOb3/wmF154YRYsWNBkuz//+c/56le/msmTJydJRowYsT3KAwAAACikDtvjJgcddFDOO++83H333Zk8eXKmTJmS/fbbL/vss086d+6c999/P//3//7fzJs3rzIjaOTIkfn4xz++PcoDAAAAKKTtEvwkyaWXXppu3brljjvuyKpVqzJ37tzMmzevcr0h8OnQoUO+/OUv56KLLtpepQEAAAAU0nYLfkqlUr785S/nc5/7XJ588sn85je/yaJFi7J8+fJ07tw5vXv3ziGHHJITTjghvXr12l5lAQAAABTWdgt+Guy+++4ZOXJkRo4cub1vDQAAANCmbJfFnQEAAADY/gQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoDrs6AJai8svvzw//elPW9Tnxz/+cf7mb/4mSbJkyZIcfvjhm+zTo0ePTJs2rclrs2fPzj333JNp06ZlyZIl6dGjRwYNGpQRI0ZkyJAhLaoNAAAAKD7BzzbUtWvXyvHLL7+8RWM99dRTueSSS1JbW1s5t3jx4kyePDmTJ0/OGWeckauuumqL7gEAAAAUi+Cnma6++uqMHj16o21+97vf5YILLkh9fX3OPffcDBo0qHLtlVdeSZL06dMnEydO3OAYpVJpvXMzZszIpZdemtra2tTU1OQb3/hGPvKRj2TBggW58847M2nSpIwfPz79+vXLyJEjN/MJAQAAgKIR/DRTx44d07Fjxw1eX7ZsWb71rW+lvr4+H/vYx/K1r32t0fUZM2YkSWpqahrNBGqOW265JatWrcree++d++67r9K/Z8+eGTNmTC6++OI8+eSTufXWW3PCCSekW7duLXw6AAAAoIgs7ryVXHXVVfnzn/+czp0757vf/W46dGicqa0b/LTEvHnz8swzzyRJzj///PVCo1KplMsvvzylUinLli3L448/vgVPAQAAABSJ4GcrmDp1ap588skkyYUXXph99tmn0fXly5dn4cKFSVoe/DSEPqVSKUOHDm2yTd++fdO/f/8kyaRJk1o0PgAAAFBcgp8tVFdXl+uvvz5Jstdee+Xss89er03DbJ9SqZROnTrlW9/6VoYOHZpBgwblsMMOy/nnn58pU6Y0Of7MmTOTJHvssUd23XXXDdZx4IEHNroXAAAAgDV+ttBPfvKTzJs3L0nyD//wD02uA9QQxrRr1y4jR45MXV1d5drSpUvz9NNP5+mnn85JJ52Ua665ptFnYq+//nqStYtCb8yee+6ZJFm0aFFqa2tTVVW1ZQ8GAAAAtHqCny1QX1+fH/3oR0nWzvb5zGc+02S7huBnzZo16devXy666KIMHjw4VVVV+cMf/pDbbrsts2bNysMPP5xu3brlyiuvrPRdunRpkmTnnXfeaC077bRTkqRcLmfFihXZZZddtvj5mrLudvKtTVVVVZO7pgHAhpTL5Vb97isC728AWqo1v7+3Rd2Cny3w5JNP5rXXXkuSjBo1Ku3bt2+y3erVq9O1a9fsu+++GT9+fKMFmocPH55PfOITOeuss/KHP/wh48ePz8knn1xZs2fVqlVJks6dO2+0lk6dOlWOG/psC7NmzdpmY29rNTU1G92ZDQD+Wm1tbaZPn76jy2jTvL8BaCnv78as8bMF7r333iTJrrvumpNOOmmD7e644448//zzeeCBB5rcyr1z584ZPXp0krXJ5MMPP1y51hAmbepfusrlcuW4XTv/swIAAABm/Gy2hQsX5vnnn0+SfOYzn2k042ZD/nqL93UNGjQovXr1yqJFi/Liiy9WzldXVydJ3n///Y2OvXr16srxtvxXsQEDBrTa9YNaa90A7DhVVVUt3pGTrcv7G4CWas3v79ra2q3+pY3gZzM1bN+eJMcee+xWGXPPPffMokWLKuv6JEm3bt2SJCtWrNho37fffjvJ2tk+3bt33yr1NKWqqsp0awDajFKp5L0HAK2M93djvgnaTE888USStbttHXzwwc3qs+7nWE1pmLXTpUuXyrl+/folSd54442N9m243rt3b596AQAAAEkEP5tl5cqVeeGFF5IkQ4cO3ej6Oy+88EKGDh2aj370o3nkkUc22G7NmjWZP39+kmTfffetnG9Y5HnhwoWVWT1Nadg5bMCAAc19DAAAAKDgBD+b4cUXX0x9fX2S5JBDDtlo2z59+uT111/P+++/n2eeeWaD7X71q1/lnXfeSZIMGTKkcr7huL6+PlOmTGmy72uvvZY5c+YkSY444ohmPwcAAABQbIKfzdAwuyZJPvrRj2607e67757DDz88SfLoo4/m97///XptFi9enO985ztJkj322KPRmkF77bVXBg8enCQZM2bMemv9lMvl3HDDDSmXy+nZs2dOOOGEzXsoAAAAoHAEP5th3rx5SdbuuNWnT59Ntr/sssvSqVOn1NfX57zzzsu9996b+fPnZ/HixZk4cWJOO+20LFy4MB06dMh111233g5hV1xxRdq1a5f58+dnxIgRmTp1apYsWZIZM2bkwgsvzFNPPZUkufjiiyu7gAEAAADY1WszvP7660nWLqTcHAMGDMhtt92WSy+9NCtXrsx3vvOdygyfBtXV1bn++uvziU98Yr3+NTU1ue666zJ69OjMmTMno0aNWq/NOeeck5EjR27G0wAAAABFJfjZDA2fW+2xxx7N7nPkkUfm0Ucfzb333ptf//rXWbBgQZK14dGQIUNy1llnZc8999xg/5NOOikDBw7M2LFjM23atLz11luprq7OoEGDMmLEiAwfPnzLHgoAAAAoHMHPZvjZz362Wf169eqVyy67LJdddtlm9e/fv3++973vbVZfAAAAoO2xxg8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAF1WFHF9DafPvb386ECRM22W706NH54he/2OhcbW1tJkyYkEceeSTz5s1LuVxOnz59Mnz48Jxzzjnp0aPHRsecPXt27rnnnkybNi1LlixJjx49MmjQoIwYMSJDhgzZkscCAAAACkjw00KvvPLKZvVbtWpVRo0aleeee67R+blz52bu3Ll5+OGHM3bs2Oy///5N9n/qqadyySWXpLa2tnJu8eLFmTx5ciZPnpwzzjgjV1111WbVBgAAABST4KcF1qxZk9mzZydJrr766hx33HEbbNuxY8dGv7/sssvy3HPPpaqqKhdddFGOO+64dOzYMVOmTMmNN96YN998MxdccEF+/vOfp7q6ulHfGTNm5NJLL01tbW1qamryjW98Ix/5yEeyYMGC3HnnnZk0aVLGjx+ffv36ZeTIkVv/wQEAAIBWyRo/LTB37ty8//77SZLBgwena9euG/xVVVVV6ffSSy/lscceS5JceeWVueCCC9K3b9/svvvuOfXUUzNu3LhUVVVl4cKFue+++9a77y233JJVq1Zl7733zn333ZdDDz00PXv2TE1NTcaMGZOjjz46SXLrrbdm5cqV2+EnAQAAALQGgp8WaPjMq7q6Ovvtt1+z+40bNy5J0qdPn5x66qnrXa+pqcnxxx+fJHnwwQcbXZs3b16eeeaZJMn555+frl27NrpeKpVy+eWXp1QqZdmyZXn88ceb/0AAAABAoQl+WmDGjBlJkkGDBqVdu+b96MrlcqZOnZokOfLII9O+ffsm2w0bNixJsnDhwsycObNyviH0KZVKGTp0aJN9+/btm/79+ydJJk2a1Ky6AAAAgOIT/LRAQ/AzYMCAPPDAA/niF7+YwYMH56CDDsr/9//9f/n+97+fpUuXNuqzYMGCrFixIkkycODADY59wAEHVI5ffvnlynFDCLTHHntk11133WD/Aw88sFGNAAAAABZ3bqb6+vrMmjUrSTJhwoRGu2slyauvvppXX301Dz30UO68884cfPDBSdbO4GnQp0+fDY7fq1evtG/fPmvWrGnU5/XXX99k3yTZc889kySLFi1KbW1tozWGtqa/fu7WpKqqKqVSaUeXAUArUi6XW/W7rwi8vwFoqdb8/t4WdQt+mum//uu/8u677yZJ6urqMmLEiJx88snZc889s3jx4kycODHjxo3LkiVLcv755+ehhx7KXnvt1WgGUPfu3Tc4focOHdKlS5esXLkyb7/9duV8Q/+dd955o/XttNNOSdb+AV+xYkV22WWXzX7WjWkIv1qjmpqa9XZbA4CNqa2tzfTp03d0GW2a9zcALeX93ZhPvZrpzTffTO/evdO+ffvceOON+cd//McMHDgwPXv2zP7775+vfe1rufnmm5Mky5cvz4033pgkWbVqVWWMTp06bfQenTt3TpLKzmHr9m+4tiHrjr3uPQEAAIC2y4yfZjr88MMzZcqU1NXVpUOHpn9sn/rUp3LUUUdl8uTJefLJJ7N8+fJGizlvappyuVxOkkYLRzf0b27fv+6/tQ0YMGCbfUa2rbXWugHYcaqqqlJTU7Ojy2jTvL8BaKnW/P6ura3d6l/aCH5aaEOhT4Nhw4Zl8uTJqa+vz8svv5zq6urKtU3NxGm4vu7snYb+684Casrq1asrx9tyOnRVVZXp1gC0GaVSyXsPAFoZ7+/GfOq1lfXu3btyvGTJksraO0kqu3s1pa6uLu+9916SpGfPnpXz3bp122TfJJV1gdq1a7fRtYQAAACAtkPw00LrflLVlHVX4O7SpUv23Xffyu8bduhqyqJFi7JmzZok/71DV5L069cvSfLGG29s9L4N13v37r1NP/UCAAAAWg8JQTN97Wtfy2GHHZZjjjlmo+3mzp1bOe7Xr1923333ygyemTNnbrDfK6+8Ujk+4IADKsf9+/dPsnZb+HV3+/prM2bMSLJ2DR4AAACARPDTbN26dcvSpUszf/78zJ8/v8k25XI5v/jFL5Ikffr0yX777ZckOfLII5MkU6ZMSX19fZN9J02alCT50Ic+1Ci8GTJkSJKkvr4+U6ZMabLva6+9ljlz5iRJjjjiiJY9GAAAAFBYgp9m+uxnP1s5vuaaa5psc/fdd1dm9YwaNaqyE9eJJ56YJJk/f34mTJiwXr+XXnopEydOTJKcddZZjXbw2muvvTJ48OAkyZgxY9Zb66dcLueGG25IuVxOz549c8IJJ2zmEwIAAABFI/hppsGDB+fYY49NkkydOjVnn312nnvuuSxZsiSzZs3K6NGjc9NNNyVJDj300Jx++umVvocffniGDh2aJLnuuuty880357XXXsvixYvz4IMP5ktf+lLq6urSt2/fRv0aXHHFFWnXrl3mz5+fESNGZOrUqVmyZElmzJiRCy+8ME899VSS5OKLL260ixgAAADQttnOvQWuv/76vPvuu5k8eXKeffbZPPvss+u1+du//dvcdttt6y2wfMMNN2TUqFGZPn167rrrrtx1112Nru+2224ZO3ZsZRevddXU1OS6667L6NGjM2fOnIwaNWq9Nuecc05Gjhy5hU8IAAAAFIngpwU6d+6cO++8M0888UQeeuihTJ8+PStWrEj37t0zYMCAfO5zn8uxxx7b6FOtBt27d8+ECRMyYcKETJw4MfPmzcvq1avTp0+fHHXUUTnvvPOy6667bvDeJ510UgYOHJixY8dm2rRpeeutt1JdXZ1BgwZlxIgRGT58+LZ8dAAAAKAVEvy0UKlUyqc//el8+tOfbnHfqqqqnHnmmTnzzDM36979+/fP9773vc3qCwAAALQ91vgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABRUhx1dQGv09NNP56GHHsof/vCHLFmyJB07dsw+++yTI488MmeeeWZ22WWX9fosWbIkhx9++CbH7tGjR6ZNm9bktdmzZ+eee+7JtGnTsmTJkvTo0SODBg3KiBEjMmTIkC1+LgAAAKBYBD8tUFdXl8svvzwTJ05sdL62tjavvPJKXnnllTzwwAMZM2ZMPvaxjzVq8/LLL2/RvZ966qlccsklqa2trZxbvHhxJk+enMmTJ+eMM87IVVddtUX3AAAAAIpF8NMCN910UyX0GTZsWL70pS+lX79+efPNN/P000/njjvuyFtvvZULLrggjzzySHr16lXp+8orryRJ+vTps15wtK5SqbTeuRkzZuTSSy9NbW1tampq8o1vfCMf+chHsmDBgtx5552ZNGlSxo8fn379+mXkyJFb+akBAACA1soaP820aNGi/PjHP06SHH/88bnjjjtyyCGHpGfPnunfv3/OP//8/PjHP06HDh2ybNmy/OAHP2jUf8aMGUmSmpqadO3adYO/qqur17v3LbfcklWrVmXvvffOfffdl0MPPTQ9e/ZMTU1NxowZk6OPPjpJcuutt2blypXb+CcBAAAAtBaCn2Z66qmnUldXlyT56le/2mSbgw46KEcddVSSZMqUKY2urRv8tMS8efPyzDPPJEnOP//8dO3atdH1UqmUyy+/PKVSKcuWLcvjjz/eovEBAACA4hL8NNObb76Zzp07Z7fddkufPn022G7vvfeutG+wfPnyLFy4MEnLg5+G0KdUKmXo0KFNtunbt2/69++fJJk0aVKLxgcAAACKS/DTTF/96lfz4osvbnJGzZ/+9KckSffu3SvnGmb7lEqldOrUKd/61rcydOjQDBo0KIcddljOP//89WYINZg5c2aSZI899siuu+66wfseeOCBje4FAAAAYHHnFurWrdsGr73xxht5+umnkySHHHJI5XxDGNOuXbuMHDmy8slYkixdujRPP/10nn766Zx00km55ppr0qHDf//P8vrrryfJRmcZJcmee+6ZZO1aRLW1tamqqmrhkzXPuruKtTZVVVVNLp4NABtSLpdb9buvCLy/AWip1vz+3hZ1C362kvr6+owePbryP9KIESMq1xqCnzVr1qRfv3656KKLMnjw4FRVVeUPf/hDbrvttsyaNSsPP/xwunXrliuvvLLSd+nSpUmSnXfeeaP332mnnZKs/QO+YsWK7LLLLlv1+RrMmjVrm4y7PdTU1KRjx447ugwAWpHa2tpMnz59R5fRpnl/A9BS3t+N+dRrK7n++uvz61//Okly7LHH5vDDD69cW716dbp27ZqBAwfmoYceynHHHZfevXtnt912y/Dhw/OTn/wkBx98cJJk/PjxmT17dqXvqlWrkiSdO3fe6P07deq0Xh8AAACgbTPjZwuVy+Vcf/31GT9+fJJk//33zzXXXNOozR133JEkqaura/QZV4POnTtn9OjR+fznP59yuZyHH344V1xxRZKkffv2SbLJKc7lcrly3K7dtsvzBgwYsM0+I9vWWmvdAOw4VVVVLd6Yga3L+xuAlmrN7+/a2tqt/qWN4GcLrF69OldccUV+/vOfJ0k+/OEPZ9y4cettud6gqdCnwaBBg9KrV68sWrQoL774YuV8dXV1kuT999/fZC0NtuV06KqqKtOtAWgzSqWS9x4AtDLe34351GszLVmyJGeddVYl9Bk4cGD+5V/+JR/60Ic2e8yGBZob1vVJ/nsx6RUrVmy079tvv51k7WyfdXcUAwAAANouwc9mmD9/fr7whS/k+eefT5IcccQRGT9+/CYXVF73c6ymNMza6dKlS+Vcv379kqzdMWxjGq737t17m37qBQAAALQeEoIWmjVrVr7whS/kT3/6U5Lk1FNPzV133bXBz7teeOGFDB06NB/96EfzyCOPbHDcNWvWZP78+UmSfffdt3K+f//+SZKFCxdWZvU0pWHnsAEDBrTkcQAAAIACE/y0wJ/+9Kece+65lU+x/uEf/iHXXHPNRtfu6dOnT15//fW8//77eeaZZzbY7le/+lXeeeedJMmQIUMq5xuO6+vrM2XKlCb7vvbaa5kzZ06StbOPAAAAABLBT7OtXr06X/3qV/PWW28lSa644op85Stf2WS/3XffvbK1+6OPPprf//7367VZvHhxvvOd7yRJ9thjjxx77LGVa3vttVcGDx6cJBkzZsx6a/2Uy+XccMMNKZfL6dmzZ0444YTNe0AAAACgcAQ/zfTAAw9UPqc65phjcsopp+Sdd97Z6K8Gl112WTp16pT6+vqcd955uffeezN//vwsXrw4EydOzGmnnZaFCxemQ4cOue6669KpU6dG977iiivSrl27zJ8/PyNGjMjUqVOzZMmSzJgxIxdeeGGeeuqpJMnFF19c2QUMAAAAwHbuzfTjH/+4cvzLX/4yv/zlLzfZZ/bs2UnWrrtz22235dJLL83KlSvzne98pzLDp0F1dXWuv/76fOITn1hvnJqamlx33XUZPXp05syZk1GjRq3X5pxzzsnIkSNb+lgAAABAgQl+mmHp0qWVxZw315FHHplHH3009957b379619nwYIFSdbuwjVkyJCcddZZle3cm3LSSSdl4MCBGTt2bKZNm5a33nor1dXVGTRoUEaMGJHhw4dvUX0AAABA8Qh+mqFnz56V2TtbolevXrnsssty2WWXbVb//v3753vf+94W1wEAAAC0Ddb4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcAAACgoAQ/AAAAAAUl+AEAAAAoKMEPAAAAQEEJfgAAAAAKSvADAAAAUFCCHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUVIcdXQAtM3v27Nxzzz2ZNm1alixZkh49emTQoEEZMWJEhgwZsqPLAwAAAD5ABD+tyFNPPZVLLrkktbW1lXOLFy/O5MmTM3ny5Jxxxhm56qqrdmCFAAAAwAeJT71aiRkzZuTSSy9NbW1tampqMn78+PzmN7/Jv/3bv2XYsGFJkvHjx+f+++/fwZUCAAAAHxSCn1billtuyapVq7L33nvnvvvuy6GHHpqePXumpqYmY8aMydFHH50kufXWW7Ny5codXC0AAADwQSD4aQXmzZuXZ555Jkly/vnnp2vXro2ul0qlXH755SmVSlm2bFkef/zxHVEmAAAA8AEj+GkFGkKfUqmUoUOHNtmmb9++6d+/f5Jk0qRJ2602AAAA4INL8NMKzJw5M0myxx57ZNddd91guwMPPDDJ2vWAAAAAAOzq1Qq8/vrrSZI+ffpstN2ee+6ZJFm0aFFqa2tTVVW1Ve5fLpcrx++++26jXcVakw4dOqRUKqWurjY9uvijD8CG1dXVZvXq1SmXy6mrq9vR5bRp3t8ANFcR3t/r/n173b+Lbwlvz1Zg6dKlSZKdd955o+122mmnJGv/cKxYsSK77LLLVrn/uv8PM2/evK0y5o72vRP67+gSAPgA+/Of5uXPO7oI1uP9DcDGFO39XVdXl06dOm3xOD71agVWrVqVJOncufNG2637B6KhDwAAANB2mfHTCrRv3z7J2sWdN2bdaWDt2m29TK9Lly4ZMGBAkv+ebg0AAABsXet+ptalS5etMqbgpxWorq5Okrz//vsbbbd69erKcceOHbfa/du1a7feFvIAAADA1rc1Pu9al0+9WoFu3bolSVasWLHRdm+//XaStUFN9+7dt3ldAAAAwAeb4KcV6NevX5LkjTfe2Gi7huu9e/feqp96AQAAAK2TdKAV6N9/7Q4WCxcurMzqacqMGTOSpLIeDwAAANC2CX5agSFDhiRJ6uvrM2XKlCbbvPbaa5kzZ06S5IgjjthepQEAAAAfYIKfVmCvvfbK4MGDkyRjxoxZb62fcrmcG264IeVyOT179swJJ5ywI8oEAAAAPmAEP63EFVdckXbt2mX+/PkZMWJEpk6dmiVLlmTGjBm58MIL89RTTyVJLr744souYAAAAEDbViqXy+UdXQTN8/DDD2f06NGpq6tr8vo555yTyy+/fDtXBQAAAHxQCX5amdmzZ2fs2LGZNm1a3nrrrVRXV2fQoEEZMWJEhg8fvqPLAwAAAD5ABD8AAAAABWWNHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgILqsKMLAGDLzZ49O/fcc0+mTZuWJUuWpEePHhk0aFBGjBiRIUOGbPa4CxYsyN13352pU6dm0aJF6datW/r3759TTjklxx133FZ8AgDg2muvzfjx43PttdfmlFNO2aKxvMOBBqVyuVze0UUAsPmeeuqpXHLJJamtrW3y+hlnnJGrrrqqxeO++OKLOeecc/LOO+80ef1Tn/pUbr755nTo4N8QAGBLTZo0KRdddFHq6+u3OPjxDgfW5VMvgFZsxowZufTSS1NbW5uampqMHz8+v/nNb/Jv//ZvGTZsWJJk/Pjxuf/++1s07htvvJHzzz8/77zzTvbdd9/84Ac/yLPPPpuJEydW/kP0iSeeyE033bTVnwkA2prJkyfnkksuSX19/RaP5R0O/DXBD0Ardsstt2TVqlXZe++9c9999+XQQw9Nz549U1NTkzFjxuToo49Oktx6661ZuXJls8f9wQ9+kGXLlmXnnXfO+PHj88lPfjK77LJL9t9//1x77bU566yzkqwNlV577bVt8mwAUHT19fW57bbb8pWvfCWrV6/eKmN6hwN/TfAD0ErNmzcvzzzzTJLk/PPPT9euXRtdL5VKufzyy1MqlbJs2bI8/vjjzRp3+fLleeihh5IkI0eOzO67775em//1v/5XunXrltra2vz0pz/dwicBgLZn6tSpOfHEE3P77benvr4+AwcO3OIxvcOBpgh+AFqphtCnVCpl6NChTbbp27dv+vfvn2Tt2gHNMW3atMq/Om5o3G7duuVv/uZvWjQuAPDfRo0aldmzZ6eqqioXX3xxbrnlli0e0zscaIrgB6CVmjlzZpJkjz32yK677rrBdgceeGCStesBtWTcdu3a5YADDtjkuH/84x+32vR0AGgrSqVShg8fnn//93/PRRddlHbttvyvZt7hQFMs4w7QSr3++utJkj59+my03Z577pkkWbRoUWpra1NVVdWscXv16rXRtg3jrlmzJn/+85+z9957N7t2AGjrHnvssfTr12+rjukdDjTFjB+AVmrp0qVJkp133nmj7XbaaackSblczooVK7b6uMnaNQUAgObb2qFP4h0ONE3wA9BKrVq1KknSuXPnjbbr1KnTen12xLgAwLblHQ40RfAD0Eq1b98+ydo1AjamXC5XjpuzfsDmjLuptgDAtucdDjRF8APQSlVXVydJ3n///Y22W3fRxo4dO26Tcdf9l0MAYMfwDgeaIvgBaKW6deuWJJtct+ftt99Osna2T/fu3Tc5bsN3/80dN0l69uy5yXEBgG3LOxxoiuAHoJVqWBTyjTfe2Gi7huu9e/du1qde++67b5LkzTffTH19/SbH7dChQ3bffffmlAwAbEPe4UBTBD8ArVT//v2TJAsXLmz0L3d/bcaMGUmSAQMGtGjc2trazJkzZ5Pj/o//8T82uUU8ALDteYcDTRH8ALRSQ4YMSZLU19dnypQpTbZ57bXXKv/hd8QRRzRr3EMPPTRdunRJkkyePLnJNitXrsy0adNaNC4AsG15hwNNEfwAtFJ77bVXBg8enCQZM2bMet/zl8vl3HDDDSmXy+nZs2dOOOGEZo1bXV2do48+Okly77335vXXX1+vzS233JJ33nknVVVV+eIXv7iFTwIAbA3e4UBTBD8ArdgVV1yRdu3aZf78+RkxYkSmTp2aJUuWZMaMGbnwwgvz1FNPJUkuvvjiyk4fDY455pgcc8wx+cY3vrHeuJdeemmqq6uzbNmyjBw5Mo8//niWLFmSefPm5aqrrsr48eOTJGeccUb22GOPbf+gAECFdzjQEqVyuVze0UUAsPkefvjhjB49OnV1dU1eP+ecc3L55Zevd75hHYBDDz208h+B6/r1r3+diy++OO+9916T4x5zzDG5+eabm7VgNACwcQsWLMiwYcOSJNdee21OOeWUDbb1DgdaosOOLgCALXPSSSdl4MCBGTt2bKZNm5a33nor1dXVGTRoUEaMGJHhw4dv1rhHHHFEfvGLX+SHP/xhpk6dmkWLFqVjx44ZMGBAPv/5z+ekk05KqVTayk8DAGwp73BgXWb8AAAAABSUuX0AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4AAAAACkrwAwAAAFBQgh8AAACAghL8AAAAABSU4AcA4ANkzZo1O7qE7aK+vn5HlwAAbYLgBwBo84YOHZr+/ftv8NegQYNy6KGH5oQTTsjVV1+dGTNmbJM6HnvssXz961/f6uM2PMfNN9/c4r633XZbpX9dXV3l/MMPP1w5/6c//anZ93vttddy7rnn5vXXX2/5gwAALSb4AQDYhNra2ixfvjyzZs3K/fffn89//vObFaJszC233JJLLrkkixcv3qrjfpDMnj07xx13XP7jP/5jR5cCAG1Ghx1dAADAB8XgwYNz9913r3e+vr4+K1asyO9+97v80z/9U954443cdddd2XvvvfP5z39+q9z7z3/+81YZZ2vr3r179t577yRJqVRqVp+G9j179mx0ftmyZXn//fe3boEAwEYJfgAA/p/27duna9euTV7baaed8tnPfjY1NTU54YQTsmrVqvzzP/9zPve5z6Vdu+JOoj7zzDNz5plntqjPk08+uY2qAQBaqrj/lQIAsA3069cvxx57bJJk0aJFefnll3dwRQAAGyb4AQBooQMPPLByvHDhwkbXVq5cmR/+8Ic59dRT8/GPfzw1NTU56qij8vWvfz1/+MMf1hurYfHkn/70p0mS3/72t5XFkRcsWNCo7cKFC3PTTTfl5JNPzmGHHZaBAwfmf/7P/5kTTzwx3//+97No0aJN1r5kyZJcffXV+eQnP5mampoMHTo03/rWt/J//+//bbL9hhZ33pi/Xtx5wYIF6d+/f6OZQ8OGDUv//v1z22235Wc/+1mlz29/+9sNjrtq1aoMHjw4/fv3zz333NOsWgCgrRP8AAC00Lpr3bRv375y/OKLL+bYY4/NTTfdlBdffDErVqzI6tWr8/rrr+eRRx7Jaaedluuvv36ztjJ/8MEH8+lPfzo//OEPM3369CxdujR1dXV5++23M3PmzNx99905/vjjN7rj2IIFC3LiiSfm/vvvzxtvvJHVq1dn4cKF+clPfpJjjz02jz76aIvr2ho+9alPpbq6Okny85//fIPtJk+enJUrV6Zdu3Y5/vjjt1d5ANCqCX4AAFpo3c+7PvzhDydJ/vSnP+X888/Pn//853Tt2jVf//rX8+STT+bZZ5/Nvffem0MPPTRJct999+Wf/umfKv2//OUv5/nnn68EGYMHD87zzz+f559/Pn369EmSvPTSSxk9enRqa2szaNCg/OAHP8jkyZPzH//xH/nXf/3XnHjiiUmS5cuX5/rrr99g3T//+c/z5ptv5txzz80vf/nL/Od//mf+6Z/+Kb169crq1avz9a9/PbNmzdqqP6sk6dOnT55//vn88Ic/rJz7xS9+keeffz5f/vKXU11dneHDhydJHn/88Q3OLGoIhQ499ND06tVrq9cJAEUk+AEAaIE5c+ZUZsZ85CMfqQQ/N910U5YtW5aqqqrce++9+dKXvpS99947u+yySw4//PDce++9lXDjnnvuyR//+MckSceOHdO1a9d06LB2z42GBaa7du1amVk0duzYlMvl7LLLLhk3blw++clPZs8998xuu+2Wj33sY/nud79bGfv3v/993nnnnQ3W/81vfjOXXXZZ+vXrl1133TXHHnts7r///nTr1i11dXWNQqmtpVQqpWvXruncuXPlXOfOndO1a9d07NgxSXLCCSckWbvzV1Pbva9YsSJPP/10kuSzn/3sVq8RAIpK8AMA8P+sWbMm77zzznq/li5dmpkzZ+aHP/xhRo4cmVWrVqVUKuXrX/96kmTp0qV54oknkiQjRozIQQcdtN7Y7du3z//+3/87VVVVKZfL+dd//ddm13XIIYfk5JNPzoUXXpju3bs32aZhRlG5XM7y5cubbLP//vs3uUPXXnvtlbPOOitJ8utf/zpvvfVWs2vbWv72b/82u+++e5K1s4H+2uOPP57Vq1enc+fO+fSnP729ywOAVst27gAA/8/vf//7HHLIIZtsV1VVlSuvvDJHHnlkpV+5XE6ydr2aDdltt93y8Y9/PM8+++xGFzH+aw2hzIbMnz8/8+bNq/x+Q59KHX300Rsc48gjj8yYMWNSX1+f559/fqNtt4V27drluOOOy7hx4/LUU09l1apV6dSpU+X6xIkTkyRDhw5Nt27dtmttANCaCX4AADahU6dO2WmnndKvX78MHjw4p5xySvr27Vu5/sYbb1SOGz792pAPf/jDefbZZxv1aa733nsvv/71rzNnzpz86U9/ymuvvZa5c+dmxYoVjdo1hFB/bb/99tvg2Pvss0/l+K93KtteTjzxxIwbNy7vvPNOpkyZUpnZ8+abb1aCMp95AUDLCH4AAP6fQw89NOPHj29xv5UrV1aOu3btutG2Xbp0SZK8++67zR6/vr4+d999d8aNG5dly5Y1ulZVVZWPfexj2XnnnStr4Gzq3k1p2FUrSd5///1m17Y19e/fPwMGDMisWbPyi1/8ohL8PProo6mvr0/Pnj3ziU98YofUBgCtleAHAGALrRuavPPOO5UFi5vSEBJtLIT5a9/73vfyox/9KEmy7777ZtiwYRkwYEA+/OEP5yMf+Ug6duyYBx98cJPBz6pVqzZZV5LstNNOza5tazvhhBMya9asTJkyJe+88066du1a2c3rM5/5TKqqqnZYbQDQGgl+AAC2UMO260kyb968fPzjH99g24a1ePbcc89mjf3nP/859913X5K169vcfvvtad++/Xrtli5dusmxFixYsMFrr776auV47733blZt28Jxxx2X73//+1m1alWmTp2agw8+ONOnT0/iMy8A2Bx29QIA2EKHHHJI2rVb+59Vjz/++AbbLV68OC+88EKS5GMf+1ijaw1bt/+1F198MfX19UmS0047rcnQJ0meffbZynFD+7/2n//5nxus7amnnkqy9tOxgw8+eIPttsSGnnFdu+++ew4//PAkyeTJkyuzmPbZZ59tVhcAFJngBwBgC+2yyy4ZPnx4kmTChAl56aWX1mtTX1+fq6++OrW1tSmVSjn55JMbXW8IdGpra5s8nyRz585t8v4PPfRQo1Dnr8do8Oyzz2bSpEnrnZ81a1b+z//5P0nWfk61rT71WvdZNlRjsvZzrySZMmVKfvWrXyUx2wcANpfgBwBgK/jGN76RnXfeObW1tTn77LMzduzYvPbaa1m6dGmmTZuWc889N0888USS5JxzzslBBx3UqH+PHj2SJLNnz84f/vCHLF26NLW1tRk8eHA6d+6cJLn99ttz//33Z8GCBfnLX/6S5557Lpdddlm++c1vNhprQwtHt2/fPpdccknuvvvuLFy4MIsXL86DDz6Ys846K6tWrUqPHj3yta99bSv/ZNZ/xmTt9uxLlixptLZQg6OPPjrV1dVZunRppkyZkiQ5/vjjt1ldAFBkgh8AgK1gr732ytixY/OhD30o77zzTr73ve9l+PDhOeyww3LmmWdWPsU699xzmwxX/uZv/ibJ2tDmtNNOy2GHHZbnn38+PXv2zOWXX55SqZT33nsvV199dYYNG5a/+7u/yxe/+MX87Gc/S8eOHfP3f//3lbHmz5/fZI1f+cpX0rlz53z/+9/P0KFD84lPfCJXXXVVli1bll122SX33HNPevXqtfV/OP/PPvvsk969eydJxowZk8MPPzzXXnvteu26dOlS2dGrXC7n4IMPbrTdPADQfIIfAICt5KCDDsovf/nLfPWrX81BBx2UnXbaKZ07d86+++6bk08+OQ8++GAuu+yydOiw/v4aRxxxRL71rW9l3333TVVVVXbZZZf85S9/SZKcfvrp+dGPfpRPfvKT6dmzZ9q3b5+uXbtm//33zxlnnJGJEyfmkksuyb777pskefLJJ5us73/8j/+Rn/70pznhhBOy6667pmPHjtlnn31y7rnn5he/+EVqamq22c8mSTp06JC77rorf/M3f5Pq6upUV1fnnXfeabJtw+deic+8AGBLlMrlcnlHFwEAAOv6zW9+k7POOitVVVV55plnsssuu+zokgCgVTLjBwCAD5xHHnkkSfLJT35S6AMAW0DwAwDAB8r8+fPz6KOPJklOPfXUHVwNALRu639gDgAA29mvfvWrzJkzJ6tWrcoDDzyQ9957LwMGDMgRRxyxo0sDgFZN8AMAwA73xhtv5Oabb678vmPHjrnmmmtSKpV2YFUA0Pr51AsAgB1uwIAB6dWrVzp37pyPfexj+dGPfpSDDjpoR5cFAK2eXb0AAAAACsqMHwAAAICCEvwAAAAAFJTgBwAAAKCgBD8AAAAABSX4AQAAACgowQ8AAABAQQl+AAAAAApK8AMAAABQUIIfAAAAgIIS/AAAAAAUlOAHAAAAoKAEPwAAAAAFJfgBAAAAKCjBDwAAAEBBCX4A4P/fjh3IAAAAAAzyt77HVxgBAMCU+AEAAACYEj8AAAAAU+IHAAAAYCpUj5ajJmrSWAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 430,
       "width": 575
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Potability', data=df_cleaned)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanceo de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting features (independent variables) and the target variable (dependent variable)\n",
    "features = df_cleaned.drop('Potability', axis=1)\n",
    "labels = df_cleaned['Potability']\n",
    "\n",
    "# Creating arrays for features and labels\n",
    "features_space = features\n",
    "labels_space = labels.values\n",
    "\n",
    "# Selecting 200 random samples from the dataset\n",
    "random_values = features_space.sample(n=10)\n",
    "X_sample = random_values.values\n",
    "y_sample = labels_space[random_values.index]\n",
    "\n",
    "# Instantiating the RandomOverSampler with random_state=0\n",
    "oversampler = RandomOverSampler(random_state=0)\n",
    "\n",
    "# Performing random oversampling to balance the dataset\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_sample, y_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divición de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (9, 9)\n",
      "X_val shape: (3, 9)\n"
     ]
    }
   ],
   "source": [
    "# Rounding the values in X_resampled to 2 decimal places\n",
    "X_resampled_rounded = np.round(X_resampled, 2)\n",
    "\n",
    "# Splitting the rounded data into training and validation sets\n",
    "# test_size=0.2 specifies that 20% of the data will be used for validation\n",
    "# random_state=42 sets the random seed for reproducibility\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled_rounded, \n",
    "                                                  y_resampled, \n",
    "                                                  test_size=0.2, \n",
    "                                                  random_state=42)\n",
    "\n",
    "# Printing the shapes of the training and validation sets\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_val shape:', X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversión de imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the numerical training data into images\n",
    "train_images = data_to_image(X_train)\n",
    "\n",
    "# Converting the numerical validation data into images\n",
    "val_images = data_to_image(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the training images to PyTorch tensors\n",
    "X_train_I = torch.from_numpy(train_images).float()\n",
    "\n",
    "# Converting the training labels to PyTorch tensors\n",
    "y_train_I = torch.from_numpy(y_train).long()\n",
    "\n",
    "# Converting the validation images to PyTorch tensors\n",
    "X_val_I = torch.from_numpy(val_images).float()\n",
    "\n",
    "# Converting the validation labels to PyTorch tensors\n",
    "y_val_I = torch.from_numpy(y_val).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating PyTorch datasets for training and validation\n",
    "train_dataset = TensorDataset(X_train_I, y_train_I)\n",
    "val_dataset = TensorDataset(X_val_I, y_val_I)\n",
    "\n",
    "# Creating data loaders for training and validation sets\n",
    "# The DataLoader class provides an iterable over the dataset, with optional shuffling and batching\n",
    "dataloaders = {'train': DataLoader(train_dataset, batch_size=32, shuffle=True),\n",
    "               'val': DataLoader(val_dataset)}\n",
    "\n",
    "# Storing the sizes of the training and validation datasets\n",
    "dataset_sizes = {'train': len(X_train),\n",
    "                 'val': len(X_val)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling: By [AHN MINJAE](https://github.com/EmjayAhn/SuperTML-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the pre-defined model to the specified device (GPU if available, otherwise CPU)\n",
    "model = model_res.to(device)\n",
    "\n",
    "# Defining the loss function (cross-entropy loss)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Defining the optimizer (Adam optimizer) and passing model parameters to be optimized\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1/1:\n",
      "----------\n",
      "train Loss: 1.0407 Acc: 0.5556\n",
      "val Loss: 1.0401 Acc: 0.6667\n",
      "\n",
      "Training completed in 0m 3s\n",
      "BEST VALIDATION ACCURACY: 0.6667\n"
     ]
    }
   ],
   "source": [
    "best_model = train_model(model, dataloaders, dataset_sizes, criterion, optimizer, device, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **T1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 97,
       "width": 515
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convertir los datos a mapas de calor en escala de grises\n",
    "train_heatmaps_bw = data_to_heatmap_bw(X_train)\n",
    "val_heatmaps_bw = data_to_heatmap_bw(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de train_heatmaps_bw: (9, 480, 640, 4)\n",
      "Forma de val_heatmaps_bw: (3, 480, 640, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Forma de train_heatmaps_bw:', train_heatmaps_bw.shape)\n",
    "print('Forma de val_heatmaps_bw:', val_heatmaps_bw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the training images to PyTorch tensors\n",
    "X_train_II = torch.from_numpy(train_heatmaps_bw).float()\n",
    "\n",
    "# Converting the training labels to PyTorch tensors\n",
    "y_train_II = torch.from_numpy(y_train).long()\n",
    "\n",
    "# Converting the validation images to PyTorch tensors\n",
    "X_val_II = torch.from_numpy(val_heatmaps_bw).float()\n",
    "\n",
    "# Converting the validation labels to PyTorch tensors\n",
    "y_val_II = torch.from_numpy(y_val).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_II = torch.from_numpy(train_heatmaps_bw).float()\n",
    "X_val_II = torch.from_numpy(val_heatmaps_bw).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_heatmaps_bw = np.mean(train_heatmaps_bw, axis=3)\n",
    "val_heatmaps_bw = np.mean(val_heatmaps_bw, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de X_train_II - batch_size: 9\n",
      "Tamaño de X_train_II - canales: 480\n",
      "Tamaño de X_train_II - altura: 640\n",
      "Tamaño de X_train_II - ancho: 4\n"
     ]
    }
   ],
   "source": [
    "print('Tamaño de X_train_II - batch_size:', X_train_II.size(0))\n",
    "print('Tamaño de X_train_II - canales:', X_train_II.size(1))\n",
    "print('Tamaño de X_train_II - altura:', X_train_II.size(2))\n",
    "print('Tamaño de X_train_II - ancho:', X_train_II.size(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convertir los mapas de calor a tensores de PyTorch\n",
    "X_train_tensor = torch.from_numpy(train_heatmaps_bw).unsqueeze(1).float()\n",
    "X_val_tensor = torch.from_numpy(val_heatmaps_bw).unsqueeze(1).float()\n",
    "\n",
    "# Crear conjuntos de datos y cargadores de datos\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_II)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_II)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Definir las capas convolucionales y de pooling\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        # Definir las capas completamente conectadas\n",
    "        self.fc1 = nn.Linear(32 * 120 * 160, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # 2 clases (potable, no potable)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Aplicar convoluciones y funciones de activación\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Aplanar la salida de las capas convolucionales\n",
    "        x = x.view(-1, 32 * 120 * 160)\n",
    "        # Aplicar capas completamente conectadas\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instanciar el modelo\n",
    "model = SimpleCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1/1:\n",
      "----------\n",
      "Train Loss: 0.6965 Acc: 0.4444\n",
      "Val Loss: 79.8803 Acc: 0.3333\n",
      "Epoch Time: 3.64 seconds\n",
      "\n",
      "BEST VALIDATION ACCURACY: 0.3333\n"
     ]
    }
   ],
   "source": [
    "import time  # Importa el módulo time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Supongo que model, train_loader, y val_loader ya están definidos\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 30\n",
    "best_val_accuracy = 0.0  # Variable para rastrear el mejor rendimiento de validación\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()  # Inicia el contador de tiempo al inicio de la época\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    # Validación del modelo\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = correct_val / total_val\n",
    "\n",
    "    epoch_time = time.time() - start_time  # Calcula el tiempo transcurrido al final de la época\n",
    "\n",
    "    print(f'EPOCH {epoch+1}/{epochs}:')\n",
    "    print('-' * 10)\n",
    "    print(f'Train Loss: {train_loss:.4f} Acc: {train_accuracy:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f} Acc: {val_accuracy:.4f}')\n",
    "    print(f'Epoch Time: {epoch_time:.2f} seconds\\n')  # Imprime el tiempo transcurrido\n",
    "\n",
    "    # Actualización del mejor rendimiento de validación\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "\n",
    "print(f'BEST VALIDATION ACCURACY: {best_val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **T2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images GG - Vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_gg = gray_gang_gg(X_train)\n",
    "val_images_gg = gray_gang_gg(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 389,
       "width": 389
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = train_images_gg[2]\n",
    "\n",
    "if isinstance(image, torch.Tensor):\n",
    "    image = image.numpy()\n",
    "\n",
    "if image.shape[0] == 3:  \n",
    "    image = np.transpose(image, (1, 3, 0))\n",
    "    \n",
    "plt.imshow(image)\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PotabilityDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones para el dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PotabilityDataset(train_images_gg, y_train, transform=transform)\n",
    "val_dataset = PotabilityDataset(val_images_gg, y_val, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función de pérdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:\n",
      "Train Loss: 0.6916200518608093\n",
      "Validation Accuracy: 0.3333333333333333\n",
      "----------\n",
      "Training completed.\n",
      "BEST VALIDATION ACCURACY: 0.3333\n"
     ]
    }
   ],
   "source": [
    "train_model_Vit(model, train_loader, val_loader, criterion, optimizer, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **T3** (Working)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA con ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = data_to_image_pca(X_train)\n",
    "val_images = data_to_image_pca(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PotabilityDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PotabilityDataset(train_images, y_train, transform=transform)\n",
    "val_dataset = PotabilityDataset(val_images, y_val, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)  # Assuming binary classification\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo de transformers\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función de pérdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_1(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    best_val_accuracy = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"Train Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            val_accuracy = correct / total\n",
    "            print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "\n",
    "        print('-' * 10)\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "    print(f\"BEST VALIDATION ACCURACY: {best_val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[93], line 10\u001b[0m, in \u001b[0;36mtrain_model_1\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\nesto\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nesto\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nesto\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:806\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    804\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 806\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_flash_attention_2:\n\u001b[0;32m    809\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nesto\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nesto\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nesto\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:129\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[1;34m(self, input_ids, input_embeds)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    input_ids (torch.Tensor):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03membeddings)\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     input_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m    131\u001b[0m seq_length \u001b[38;5;241m=\u001b[39m input_embeds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Setting the position-ids to the registered buffer in constructor, it helps\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# when tracing the model without passing position-ids, solves\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# isues similar to issue #5664\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nesto\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nesto\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nesto\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nesto\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "train_model_1(model, train_loader, val_loader, criterion, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
